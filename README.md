# Generative AI References
- 최신 업데이트를 팔로우하기 위해 arxiv 버전의 링크를 권합니다

# Generate Dataset with LLMs
- **[Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707)**
- **[WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)**
- **[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)**

# Finetune LLMs
- **[LoRA Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)**
- **[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)**

# Datasets
- **Generated**: LLM으로 생성한 데이터
- **Created**: 사람이 생성한 데이터
- **Translated**: 다른 언어를 한국어로 번역한 데이터

## Korean 
- 8k, Dialogue, Generated: [junelee/remon_without_nsfw](https://huggingface.co/datasets/junelee/remon_without_nsfw)
- 0.8k, Dialogue, Translated (Original [SharedGPT](https://huggingface.co/datasets/64bits/lima_vicuna_format)): [changpt/ko-lima-vicuna](https://huggingface.co/datasets/changpt/ko-lima-vicuna)


## English

# Korean LLMs
- **[Polyglot: Large Language Models of Well-balanced Competence in Multi-languages](https://github.com/EleutherAI/polyglot)**

# Code References & Framework
- [wawawario2/long_term_memory](https://github.com/wawawario2/long_term_memory)
